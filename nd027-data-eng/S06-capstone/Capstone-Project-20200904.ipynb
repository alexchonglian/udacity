{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Analyzing NBA player and team stats with Spark/Redshift\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope\n",
    "\n",
    "The goal of this capstone project is to:\n",
    "* Collect NBA player data, season stats data, and team data.\n",
    "* Extract data from S3 files (in csv, json, txt format) to Spark DataFrame.\n",
    "* Clean and transform data using Spark, load data back to S3 in parquet format.\n",
    "* Load them to Redshift tables.\n",
    "* Analyze NBA dataset for more insights using SQL. I will try to write some queries to answer questions, e.g. \n",
    "  * What is the best winning percentage team?\n",
    "  * Which team have the most star players?\n",
    "  * Top 10 coach in history?\n",
    "  * The most efficient player? The best 3 point shooter? The best defensive player in terms of block and steal?\n",
    "  * How does the game evolve over time? for example, shooting more 3 pointers? or focusing more on defense?\n",
    "\n",
    "### Describe and Gather Data \n",
    "\n",
    "#### DataSet 1: NBA player and player stats per season.\n",
    "https://www.kaggle.com/drgilermo/nba-players-stats\n",
    "\n",
    "This dataset contains aggregate individual statistics for 67 NBA seasons. from basic box-score attributes such as points, assists, rebounds etc., to more advanced money-ball like features such as Value Over Replacement.\n",
    "The data was scraped from [basketball-reference](https://www.basketball-reference.com/)\n",
    "\n",
    "* **Players.csv**: \n",
    "This file basic player information, e.g. weight, height, college.\n",
    "Since all the play names in this file are unique, I will mainly use this csv file to create player table. Sample data:\n",
    "|Id | Player | height | weight | collage | born | birth_city | birth_state |\n",
    "|:-|:-|:-|:-|:-|:-|:-|:-|\n",
    "|2590 | Vince Carter | 198 | 99 | University of North Carolina | 1977 | Daytona Beach | Florida |\n",
    "\n",
    "* **player_data.json**: \n",
    "This file contains extra player information, e.g. more accurate birth date.\n",
    "Since this file contains duplicate NBA players names, as I show in Step 2: Explore and Assess the Data, for this project, I will only use it to augment the birth date information in the player table.\n",
    "```\n",
    "    \"4290\": {\n",
    "        \"name\": \"Russell Westbrook\",\n",
    "        \"year_start\": \"2009\",\n",
    "        \"year_end\": \"2018\",\n",
    "        \"position\": \"G\",\n",
    "        \"height\": \"6-3\",\n",
    "        \"weight\": \"200\",\n",
    "        \"birth_date\": \"November 12, 1988\",\n",
    "        \"college\": \"University of California, Los Angeles\"\n",
    "    }\n",
    "```\n",
    "\n",
    "* **Seasons_Stats.csv**: \n",
    "This file contains NBA player stats over all the seasons, from 1950 to 2015. \n",
    "The column names are abbreviated, e.g. **3P%** - 3-Point Field Goal Percentage (available since the 1979-80 season in the NBA); the formula is 3P / 3PA.\n",
    "More detailed column description can be found in [glossary](https://www.basketball-reference.com/about/glossary.html)\n",
    "Maybe expand them to more human readable format when creating tables on Redshift. Sample data:\n",
    "\n",
    "| Id | Year | Player | Pos | Age | Tm | ... | AST | STL | BLK | TOV | PF | PTS |\n",
    "|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n",
    "| 16746 | 2004 | LeBron James | SG | 19 | CLE | ... | 465 | 130 | 58 | 273 | 149 | 1654 |\n",
    "\n",
    "\n",
    "\n",
    "#### DataSet 2: NBA team record per season.\n",
    "https://www.kaggle.com/boonpalipatana/nba-season-records-from-every-year\n",
    "This dataset contains every season record for each NBA teams from 73 seasons (#wins, #losses, standing, playoff result, and more).\n",
    "* **Team_Records.csv**:\n",
    "This file contains every season record for each NBA team from 73 seasons, from 1946 to 2017.\n",
    "\n",
    "| Season | Lg | Team | W | L | W/L% | Finish | ...  | Coaches | Top WS |\n",
    "|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n",
    "| 2004-05 | NBA | Boston Celtics* | 45 | 37 | 0.549 | 1 | ...       | D. Rivers (45-37) | P. Pierce (11.2) |\n",
    "| 2003-04 | NBA | Boston Celtics* | 36 | 46 | 0.439 | 4 | ...       | J. O'Brien (22-24) J. Carroll (14-22) | P. Pierce (7.1) |\n",
    "\n",
    "Multiple coaches can coach the same team in a season, thus I need to parse \"J. O'Brien (22-24) J. Carroll (14-22),P. Pierce (7.1)\" into a list of coaching history.\n",
    "\n",
    "\n",
    "#### DataSet 3: NBA team timeline.\n",
    "http://www.shrpsports.com/nba/explain.htm\n",
    "\n",
    "This is a webpage that contains team name, team abbrevation, start and end season.\n",
    "Dataset 1 (player stats) uses team abbrevation, while dataset 2 (team stats) uses full team name, establishing the mapping between abbrev and full name (e.g. GSW => Golden State Warrior) requires a lots of manual work, I hope to automate joining two tables using information in this webpage.\n",
    "\n",
    "* **team-abbrevation.txt**:\n",
    "This files contains city, abbrevation, team name and time.\n",
    "\n",
    "```\n",
    "Baltimore    \tBal\tBaltimore Bullets (2nd team) (1963-64 - 1972-73)\n",
    "Boston       \tBos\tBoston Celtics (1946-47 - present)\n",
    "Brooklyn     \tBkn\tBrooklyn Nets (2012-13 - present)\n",
    "Buffalo      \tBuf\tBuffalo Braves (1970-71 - 1977-78)\n",
    "Capital      \tCap\tCapital Bullets (1973-74)\n",
    "Charlotte    \tCha\tCharlotte Hornets (1988-89 - 2001-02, 2014-15 - present)\n",
    "Cha Bobcats  \tChB\tCharlotte Bobcats (2004-05 - 2013-14)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col, isnan, when, count, trim, desc, sum, asc\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import countDistinct, explode, split, concat_ws, collect_list\n",
    "from pyspark.sql.types import (\n",
    "    StructType as R,\n",
    "    StructField as Fld,\n",
    "    DoubleType as Dbl,\n",
    "    StringType as Str,\n",
    "    IntegerType as Int,\n",
    "    DateType as Date,\n",
    "    TimestampType as Ts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['KEY']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load players.csv\n",
    "playerSchema = R([\n",
    "    Fld(\"id\", Int()),\n",
    "    Fld(\"Player\", Str()),\n",
    "    Fld(\"height\", Int()),\n",
    "    Fld(\"weight\", Int()),\n",
    "    Fld(\"collage\", Str()),\n",
    "    Fld(\"born\", Int()),\n",
    "    Fld(\"birth_city\", Str()),\n",
    "    Fld(\"birth_state\", Str()),\n",
    "])\n",
    "dfPlayer = spark.read.csv(\"s3a://udacity-data-eng-capstone/Players.csv\", header=True, schema=playerSchema)\n",
    "#dfPlayer.printSchema()\n",
    "dfPlayer.show(5)\n",
    "print(\"count = \", dfPlayer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename dfPlayer.Player column as dfPlayer.name\n",
    "# rename dfPlayer.collage column as dfPlayer.college\n",
    "dfPlayer = dfPlayer.withColumn(\n",
    "    \"name\", dfPlayer.Player\n",
    ").drop(\n",
    "    \"Player\"\n",
    ").withColumn(\n",
    "    \"college\", dfPlayer.collage\n",
    ").drop(\n",
    "    \"collage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPlayer.select(\"name\").where(dfPlayer.name.like('%Iverson%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Need to clean up player name, some hall of famer have star in their names \"Yao Ming*\", \"Allen Iverson*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# trim * in name\n",
    "dfPlayer = dfPlayer.withColumn(\"name\", F.regexp_replace(\"name\", \"\\*+\", \"\"))\n",
    "#dfPlayer = dfPlayer.withColumn(\"name\", F.regexp_replace(\"name\", \"([\\w+\\s]+)\", \"$1\")) #figure out capture group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# verify names are trimmed\n",
    "dfPlayer.select(\"name\").where(dfPlayer.name.like('%Iverson%')).show()\n",
    "dfPlayer.select(\"name\").where(dfPlayer.name.like('%Yao Ming%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# player with the same name?\n",
    "dfPlayer.groupBy(\"name\").count().filter(\"count > 1\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# inspect player with identical names\n",
    "dfPlayer.where(dfPlayer.name == 'Patrick Ewing').show(truncate=False)\n",
    "dfPlayer.where(dfPlayer.name == 'Gary Payton').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Since they have identical record, except id, so its safe to drop them\n",
    "print(\"before delete, num rows\", dfPlayer.count())\n",
    "dfPlayer = dfPlayer.dropDuplicates([\"name\", \"born\"])\n",
    "print(\"after  delete, num rows\", dfPlayer.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load player_data2.json in dataFrame \"dfplayExtra\", this file contains duplicate player names, also the birth date is more accurate than Players.csv\n",
    "\n",
    "* will parse player birth from dfPlayerExtra, and add extra colums (birth_day, birth_month, birth_year) to dfPlayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load player_data2.json\n",
    "playerExtraSchema = R([\n",
    "    Fld(\"name\", Str()),\n",
    "    Fld(\"year_start\", Int()),\n",
    "    Fld(\"year_end\", Int()),\n",
    "    Fld(\"position\", Str()),\n",
    "    Fld(\"height\", Str()),\n",
    "    Fld(\"weight\", Int()),\n",
    "    Fld(\"birth_date\", Str()),\n",
    "    Fld(\"college\", Str()),\n",
    "])\n",
    "# json file was generated by `df.to_json('player_data2.json', orient='records', indent=4)`\n",
    "dfPlayerExtra = spark.read.option(\"multiline\", \"true\").json(\n",
    "    \"s3a://udacity-data-eng-capstone/player_data2.json\"\n",
    ")\n",
    "#dfPlayerExtra.printSchema()\n",
    "dfPlayerExtra.show(5)\n",
    "print(\"count = \", dfPlayerExtra.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPlayerExtra = dfPlayerExtra.withColumn(\n",
    "    \"name\", F.regexp_replace(\"name\", \"\\*+\", \"\") # trim * in name\n",
    ").withColumn(\n",
    "    \"birth_date_split\", F.split(F.regexp_replace(\"birth_date\", \",\", \"\"), \" \")\n",
    ")\n",
    "\n",
    "dfPlayerExtra = dfPlayerExtra.withColumn(\n",
    "    \"birth_month\", dfPlayerExtra.birth_date_split.getItem(0) # need to convert Jan=>1\n",
    ").withColumn(\n",
    "    \"birth_day\",   dfPlayerExtra.birth_date_split.getItem(1).cast(Int())\n",
    ").withColumn(\n",
    "    \"birth_year\",  dfPlayerExtra.birth_date_split.getItem(2).cast(Int())\n",
    ").drop(\n",
    "    \"birth_date_split\"\n",
    ").drop(\n",
    "    \"birth_date\"\n",
    ").dropna(\n",
    "    subset=[\"birth_year\", \"birth_month\", \"birth_day\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPlayerExtra.select([\"name\", \"birth_year\", \"birth_month\", \"birth_day\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find all distinct months\n",
    "dfPlayerExtra.select(\"birth_month\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert month Str=>Int, e.g. Jan=>1\n",
    "map_month = {\n",
    "    \"July\":         7,\n",
    "    \"November\":     11,\n",
    "    \"February\":     2,\n",
    "    \"January\":      1,\n",
    "    \"March\":        3,\n",
    "    \"October\":      10,\n",
    "    \"May\":          5,\n",
    "    \"August\":       8,\n",
    "    \"April\":        4,\n",
    "    \"June\":         6,\n",
    "    \"December\":     12,\n",
    "    \"September\":    9,\n",
    "}\n",
    "\n",
    "def translate(mapping):\n",
    "    def translate_(col):\n",
    "        return mapping.get(col, col)\n",
    "    return udf(translate_, Int())\n",
    "\n",
    "dfPlayerExtra = dfPlayerExtra.withColumn(\"birth_month\", translate(map_month)(\"birth_month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check translate is successful\n",
    "dfPlayerExtra.select(\"birth_month\").dropDuplicates().show()\n",
    "dfPlayerExtra.show(2)\n",
    "#dfPlayerExtra.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add column birth_month (timestamp), will later separate (year, month, day, ts) to a dim table later, when loading to Redshift\n",
    "from datetime import datetime\n",
    "\n",
    "def translate():\n",
    "    def translate_(y, m, d):\n",
    "        return datetime(y, m, d)\n",
    "    return udf(translate_, Ts())\n",
    "\n",
    "dfPlayerExtra = dfPlayerExtra.withColumn(\"birth_ts\", translate()(\"birth_year\",\"birth_month\", \"birth_day\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check add column successfully\n",
    "dfPlayerExtra.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uniqPlayer = dfPlayer.select(\"name\").dropDuplicates().collect()\n",
    "uniqPlayerExtra = dfPlayerExtra.select(\"name\").dropDuplicates().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# players in extra, not in orig table\n",
    "diff1 = set(uniqPlayerExtra) - set(uniqPlayer)\n",
    "print(len(diff1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# players in orig table, not in extra\n",
    "# dfPlayer p1 left join dfPlayerExtra p2 on p1.name = p2.name, how many rows will have null value on p2\n",
    "diff2 = set(uniqPlayer) - set(uniqPlayerExtra)\n",
    "print(len(diff2))\n",
    "print(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print the count of null for each columns\n",
    "dfPlayer.select([count(when(col(c).isNull(), c)).alias(c) for c in dfPlayer.columns]).show()\n",
    "dfPlayerExtra.select([count(when(col(c).isNull(), c)).alias(c) for c in dfPlayerExtra.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfJoinPlayer = dfPlayer.join(\n",
    "    dfPlayerExtra,\n",
    "    (dfPlayer.name == dfPlayerExtra.name) & (dfPlayer.born == dfPlayerExtra.birth_year),\n",
    "    \"left\"\n",
    ").drop(\n",
    "    dfPlayer.born\n",
    ").drop(\n",
    "    dfPlayerExtra.name\n",
    ").drop(\n",
    "    dfPlayerExtra.college\n",
    ").drop(\n",
    "    dfPlayerExtra.height\n",
    ").drop(\n",
    "    dfPlayerExtra.weight\n",
    ")\n",
    "print(\"dfPlayer      count = \", dfPlayer.count())\n",
    "print(\"dfPlayerExtra count = \", dfPlayerExtra.count())\n",
    "print(\"dfJoinPlayer  count = \", dfJoinPlayer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfJoinPlayer.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create dfBirthTime dataframe and save to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfBirthTime = dfJoinPlayer.select([\"birth_month\", \"birth_day\", \"birth_year\", \"birth_ts\"]).dropDuplicates().dropna(\"any\")\n",
    "dfBirthTime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfBirthTime.write.parquet(\"s3a://udacity-data-eng-capstone-parquet/dimBirthTime/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "teamStatsSchema = R([\n",
    "    Fld(\"Season\", Str()),\n",
    "    Fld(\"Lg\", Str()),\n",
    "    Fld(\"Team\", Str()),\n",
    "    Fld(\"W\", Int()),\n",
    "    Fld(\"L\", Int()),\n",
    "    Fld(\"WoLpc\", Dbl()), # W/L%\n",
    "    Fld(\"Finish\", Int()),\n",
    "    Fld(\"SRS\", Dbl()),\n",
    "    Fld(\"Pace\", Dbl()),\n",
    "    Fld(\"Rel_Pace\", Dbl()),\n",
    "    Fld(\"ORtg\", Dbl()),\n",
    "    Fld(\"Rel_ORtg\", Dbl()),\n",
    "    Fld(\"DRtg\", Dbl()),\n",
    "    Fld(\"Rel_DRtg\", Dbl()),\n",
    "    Fld(\"Playoffs\", Str()),\n",
    "    Fld(\"Coaches\", Str()),\n",
    "    Fld(\"Top WS\", Str()),\n",
    "])\n",
    "dfTeamStats = spark.read.csv(\n",
    "    \"s3a://udacity-data-eng-capstone/Team_Records.csv\",\n",
    "    header=True, schema=teamStatsSchema,\n",
    ")\n",
    "#dfTeamStats.printSchema()\n",
    "dfTeamStats.show(5)\n",
    "print(\"count = \", dfTeamStats.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "playerStatsSchema = R([\n",
    "    Fld(\"Id\",       Int()),\n",
    "    Fld(\"Year\",     Int()),\n",
    "    Fld(\"Player\",   Str()),\n",
    "    Fld(\"Pos\",      Str()),\n",
    "    Fld(\"Age\",      Int()),\n",
    "    Fld(\"Tm\",       Str()),\n",
    "    Fld(\"G\",        Int()),\n",
    "    Fld(\"GS\",       Int()),\n",
    "    Fld(\"MP\",       Int()),\n",
    "    Fld(\"PER\",      Dbl()),\n",
    "    Fld(\"TS%\",      Dbl()),\n",
    "    Fld(\"3PAr\",     Dbl()),\n",
    "    Fld(\"FTr\",      Dbl()),\n",
    "    Fld(\"ORB%\",     Dbl()),\n",
    "    Fld(\"DRB%\",     Dbl()),\n",
    "    Fld(\"TRB%\",     Dbl()),\n",
    "    Fld(\"AST%\",     Dbl()),\n",
    "    Fld(\"STL%\",     Dbl()),\n",
    "    Fld(\"BLK%\",     Dbl()),\n",
    "    Fld(\"TOV%\",     Dbl()),\n",
    "    Fld(\"USG%\",     Dbl()),\n",
    "    Fld(\"blanl\",    Str()),\n",
    "    Fld(\"OWS\",      Dbl()),\n",
    "    Fld(\"DWS\",      Dbl()),\n",
    "    Fld(\"WS\",       Dbl()),\n",
    "    Fld(\"WS/48\",    Dbl()),\n",
    "    Fld(\"blank2\",   Str()),\n",
    "    Fld(\"OBPM\",     Dbl()),\n",
    "    Fld(\"DBPM\",     Dbl()),\n",
    "    Fld(\"BPM\",      Dbl()),\n",
    "    Fld(\"VORP\",     Dbl()),\n",
    "    Fld(\"FG\",       Int()),\n",
    "    Fld(\"FGA\",      Int()),\n",
    "    Fld(\"FG%\",      Dbl()),\n",
    "    Fld(\"3P\",       Int()),\n",
    "    Fld(\"3PA\",      Int()),\n",
    "    Fld(\"3P%\",      Dbl()),\n",
    "    Fld(\"2P\",       Int()),\n",
    "    Fld(\"2PA\",      Int()),\n",
    "    Fld(\"2P%\",      Dbl()),\n",
    "    Fld(\"eFG%\",     Dbl()),\n",
    "    Fld(\"FT\",       Int()),\n",
    "    Fld(\"FTA\",      Int()),\n",
    "    Fld(\"FT%\",      Dbl()),\n",
    "    Fld(\"ORB\",      Int()),\n",
    "    Fld(\"DRB\",      Int()),\n",
    "    Fld(\"TRB\",      Int()),\n",
    "    Fld(\"AST\",      Int()),\n",
    "    Fld(\"STL\",      Int()),\n",
    "    Fld(\"BLK\",      Int()),\n",
    "    Fld(\"TOV\",      Int()),\n",
    "    Fld(\"PF\",       Int()),\n",
    "    Fld(\"PTS\",      Int()),\n",
    "])\n",
    "\n",
    "dfPlayerStats = spark.read.csv(\n",
    "    \"s3a://udacity-data-eng-capstone/Seasons_Stats.csv\",\n",
    "    header=True, schema=playerStatsSchema)\n",
    "\n",
    "#dfPlayerStats.printSchema()\n",
    "dfPlayerStats.show(5)\n",
    "print(\"count = \", dfPlayerStats.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "### 3.3 Create Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\": [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \n",
    "                        \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\": [ DWH_CLUSTER_TYPE ,  DWH_NUM_NODES ,  DWH_NODE_TYPE ,  DWH_CLUSTER_IDENTIFIER , \n",
    "                         DWH_DB ,  DWH_DB_USER ,  DWH_DB_PASSWORD ,  DWH_PORT ,  DWH_IAM_ROLE_NAME ]\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"region_name\": \"us-west-2\",\n",
    "    \"aws_access_key_id\": KEY,\n",
    "    \"aws_secret_access_key\": SECRET\n",
    "}\n",
    "\n",
    "ec2 = boto3.resource('ec2', **args)\n",
    "s3 = boto3.resource('s3', **args)\n",
    "iam = boto3.client('iam', **args)\n",
    "redshift = boto3.client('redshift', **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3bucket =  s3.Bucket(\"udacity-data-eng-capstone-parquet\") # private\n",
    "\n",
    "s3_data = iter(s3bucket.objects.filter(Prefix=\"dimBirthTime/\"))\n",
    "for _ in range(5): print(next(s3_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('1.2 Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # parameters for hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        # parameters for identifiers & credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        # parameter for role (to allow s3 access)\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# wait till cluster status is availabe\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Print and copy them to dwh.cfg, erase before submitting or pushing to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
